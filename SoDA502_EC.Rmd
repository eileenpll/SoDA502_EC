---
title: "SoDA 502 Emotional Contagion"
output: html_notebook
---

1. Emotion Detection

```{r}
# load packages
#setwd("~")

#install.packages(c("readr","jsonlite","plyr","dplyr","tidyr","tidytext"))
library(readr)
library(jsonlite)
library(plyr)
library(dplyr)
library(tidyr)
library(tidytext)
```

Functions
```{r}
## function that converts multiple json to df
json2df <- function(string) {
    lst <- head(
            strsplit(
                    gsub('\\}}\n','\\}}x!x!', string),'x!x!')[[1]],-1) %>%
            as.list()
    # replace the line below to ~ for the ease of testing code
    # df <- lapply(lst[1:100],fromJSON)
    df <- lapply(lst, fromJSON) %>%
            lapply(., function(x) {
                x[sapply(x, is.null)] <- NA; unlist(x)}) %>%
            lapply(., function(x) {
                do.call("data.frame", as.list(x))}
                ) %>%
            rbind.fill() %>%
            tibble::rowid_to_column("ID")
    return(df)
    }

# credit to https://gist.github.com/CateGitau/05e6ff80b2a3aaa58236067811cee44e
# clean tweets
cleanedText <- function(x) {
    x<-gsub("http[[:alnum:][:punct:]]*", "", x)  ## Remove URLs
    x<-gsub('\\b+RT', '', x) ## Remove RT
    x<-gsub('#\\S+', '', x) ## Remove Hashtags
    x<-gsub('@\\S+', '', x) ## Remove Mentions
    x<-gsub('[[:cntrl:]]', '', x) ## Remove Controls and special characters
    x<-gsub("\\d", '', x) ## Remove Controls and special characters
    x<-gsub('[[:punct:]]', '', x) ## Remove Punctuations
    x<-gsub("^[[:space:]]*","",x) ## Remove leading whitespaces
    x<-gsub("[[:space:]]*$","",x) ## Remove trailing whitespaces
    x<-gsub(' +',' ',x) ## Remove extra whitespaces
    return(x)
    }

EmotionDetection <- function(df) {
    # tokenize text
    df.token <- df %>%
        select(c("ID","text")) %>%
        mutate(text = as.character(text)) %>%
        mutate(
            text = cleanedText(text)
        ) %>%
        unnest_tokens(word, text)
    # sentiment analysis
    df.sentiment <- df.token %>%
        inner_join(get_sentiments("nrc"), by = "word") %>%
        filter(sentiment %in% c("fear","anger","sadness",
                                "disgust", "negative")) %>%
        group_by(ID,sentiment) %>%
        dplyr::summarise(count=n()) %>%
        # long to wide
        spread(sentiment,count,fill = 0) %>%
        .[c("ID","anger","disgust","fear","sadness","negative")] %>%
        mutate(
            anger_binary = ifelse(anger>0,1,0),
            disgust_binary = ifelse(disgust>0,1,0),
            fear_binary = ifelse(fear>0,1,0),
            sadness_binary = ifelse(sadness>0,1,0),
            negative_binary = ifelse(negative>0,1,0)
        )
    }
```

Read in data
```{r}
fileName <- "tm_tweets_merge.txt"
string <- read_file(fileName)
df <- json2df(string)

df.subset <- df %>%
    dplyr::select(c("ID", "text","retweet_count","favorite_count",
             "user.followers_count","user.location",
             "user.description","user.friends_count",
             "coordinates.coordinates1","coordinates.coordinates2",
             "created_at","place.full_name","place.name")
           ) %>%
    plyr::rename(c("coordinates.coordinates1"="long",
             "coordinates.coordinates2"="lat",
             "user.followers_count"="followers",
             "user.friends_count"="friends")) %>%
    mutate(lat = as.numeric(levels(lat))[lat],
           long = as.numeric(levels(long))[long],
           time = as.POSIXct(
               created_at, tz = "GMT", format = "%a %b %e %H:%M:%S %z %Y")
           ) %>%
    mutate(date = as.Date(time))
    
df.emotion <- EmotionDetection(df.subset) %>%
    left_join(df.subset,., by = "ID") 
df.emotion[is.na(df.emotion)] <- 0

save(df, df.emotion, file = "df_emotion.RData")
```

2. Mapping Emotions

```{r}
rm(list = ls())

#install.packages(c("ggmap","ggplot2","lubridate","maps"),quiet=TRUE)
library(lubridate)
library(maps)
#library(ggmap)
library(ggplot2)
library(dplyr)
library(sp)
library(maptools)
library(tigris)
```

```{r}
load("df_emotion.RData")

# remove data from Alaska, Hawaii & Puerto Rico
df.emotion.con <- 
    df.emotion %>% 
    filter(long > -130 & lat < 50 & lat > 20)
```


```{r}
# create a US basemap 
us_basemap <- ggplot() +
  borders(database = "state",regions = , colour = "gray85", fill = "gray80")

# any spatial temporal patterns of all tweets?
time1 <- as.numeric(df.emotion.map$time)
map_tweets <- 
    us_basemap + 
    geom_point(data = df.emotion.map, aes(
        x = long, y = lat, colour = time1),alpha = .5, size = 0.1) +
    scale_size_continuous(range = c(1, 8),
                          breaks = c(250, 500, 750, 1000)) +
    scale_colour_gradient2(low = "#3288bd", mid = "#fee08b", high = "#d53e4f", midpoint = median(time1, rm.na = TRUE)) +
    labs(title = "Tweet Locations after the Shooting of Trayvon Martin")
ggsave("map_tweets.pdf", width = 10, height = 7)

EmotionMap <- function(df, emotion, col_low = NA, col_high = NA) {
    df %>% dplyr::filter(emotion > 0)
    Emotion <- df[,emotion]
    map <- us_basemap + 
        geom_point(data = df, aes(
        x = long, y = lat,colour = Emotion), alpha = .5, size = 0.1) +
        scale_size_continuous(
            range = c(1, 8), breaks = c(250, 500, 750, 1000)) +
        scale_colour_gradient(
            low = ifelse(is.na(col_low),"#3288bd", col_low), 
                              high = ifelse(
                                  is.na(col_high),"#d53e4f", col_high)) +
        labs(
            title = "Tweet Locations after the Shooting of Trayvon Martin")
    return(map)
    }

EmotionMap(df.emotion.map, "fear")

df.discrete.emo <- df.emotion.map %>% filter(!emo_type %in% c("mixed", "none", "negative"))

# mapNegEmo <- us_basemap + 
#     geom_point(data = df.discrete.emo, aes(
#         x = long, y = lat,colour = emo_type), alpha = .5, size = 1) +
#     coord_fixed(1.3) +
#     scale_size_continuous(
#         range = c(1, 8), breaks = c(250, 500, 750, 1000)) +
#     scale_colour_brewer(type = "qual", palette = "Set1")
```

3. Spatial Analysis

Aggregate data by date and county
```{r}
# load the shapefile of US counties
# counties_shp <- tigris::counties(state = NULL, cb = FALSE, resolution = "500k", year = NULL)
# save(counties_shp, file = "counties.RData")
load("counties.RData")
counties_shp <- spTransform(counties_shp, CRS("+proj=longlat +datum=WGS84"))

coordinates(df.emotion.con) <- ~long+lat
proj4string(df.emotion.con) <- CRS("+proj=longlat +datum=WGS84")

#overlay the spatial points onto the spatial polygons
overlap_set <- sp::over(df.emotion.con, counties_shp)
df.emotion.con <- as.data.frame(df.emotion.con)
counties_df <- cbind(df.emotion.con, overlap_set)

# aggregate tweets by county
agg_dat <- plyr::count(counties_df, c('GEOID'))
agg_dat$GEOID <- as.factor(agg_dat$GEOID)

# create the data structure needed to create a plot
sp_f <- fortify(counties_shp)
counties_shp$id <- row.names(counties_shp)
counties_shp@data <- left_join(counties_shp@data, agg_dat, by = (GEOID = "GEOID"))
sp_f <- left_join(sp_f, counties_shp@data[,c(18,19)]) %>%
  filter(long > -130 & long < -60 & lat < 50 & lat > 20) 
sp_f$freq[is.na(sp_f$freq)] <- 0

# plotting
us <- states() %>% spTransform(CRS("+proj=longlat +datum=WGS84"))
us <- fortify(us) %>%
  filter(long > -130 & long < -60 & lat < 50 & lat > 20) 

obs_by_county <- ggplot() +
    geom_polygon(data = sp_f, aes(long, lat, group = group, fill = freq)) +
    #coord_equal() +
    coord_fixed(1.3) +
    labs(fill = "No. of \nTweets")+
    geom_polygon(data=sp_f,aes(long,lat, group = group),
                 colour = "gray85", fill = NA, size = 0.2) +
    geom_polygon(data = us, aes(long, lat, group = group), colour = "gray45", fill = NA, size = 0.2) +
    ggtitle("Tweets mapped by county")+
    #scale_fill_gradient(low = "lightblue", high = "darkblue")
    #scale_fill_distiller(palette = "Spectral")
    scale_fill_distiller(palette = "Reds", direction = 1)

pdf(file="tweet_county.pdf", height=4, width=5.2)
obs_by_county
dev.off()

# calculate /day:  
# 1) number of tweets per county
# 2) number of tweets for a given emotion per county
# 3) number of emotional words per county
emo.by_timeNcounty <- counties_df %>%
    group_by(date, GEOID) %>%
    dplyr::summarise(
        n = n(),
        fear_t = sum(fear_binary),
        anger_t = sum(anger_binary),
        sadness_t = sum(sadness_binary),
        disgust_t = sum(disgust_binary),
        negative_t = sum(negative_binary),
        fear_w = sum(fear),
        anger_w = sum(anger),
        sadness_w = sum(sadness),
        disgust_w = sum(disgust),
        negative_w = sum(negative))

plot(emo.by_timeNcounty$date, emo.by_timeNcounty$n,
     xlab="Date", ylab="Number of Tweets")

ggplot(emo.by_timeNcounty, aes(x = date)) + 
    geom_point(aes(y = fear_t, colour = "fear")) + 
    geom_point(aes(y = disgust_t, colour = "disgust")) +
    geom_point(aes(y = sadness_t, colour = "sadness")) +
    geom_point(aes(y = anger_t, colour = "anger")) +
    ylab("Frequency") + xlab("Time") + labs(fill = "Emotions")

ggplot(emo.by_timeNcounty, aes(x = date)) + 
    geom_point(aes(y = fear_w, colour = "fear")) + 
    geom_point(aes(y = disgust_w, colour = "disgust")) +
    geom_point(aes(y = sadness_w, colour = "sadness")) +
    geom_point(aes(y = anger_w, colour = "anger")) +
    ylab("Word Count") + xlab("Time") + labs(fill = "Emotions")

```

Spatial autocorrelation
```{r}
library(spdep, quietly = TRUE)

# filter out tweets posted during and after the trial 
# aggregate by GEOID (counties)
emo.by_county <- emo.by_timeNcounty %>% 
    filter(date >= as.Date("2013-07-12")) %>%
    group_by(GEOID) %>%
    dplyr::summarise(
        n = sum(n),
        fear = sum(fear),
        anger = sum(anger),
        sadness = sum(sadness),
        disgust = sum(disgust),
        negative = sum(negative))

# map tweets onto spatial polygons
tweet.df <- left_join(counties_shp@data, emo.by_county, by = (GEOID = "GEOID"))
tweet.df[, 19:25][is.na(tweet.df[, 19:25])] <- 0
tweet.polygon <- SpatialPolygonsDataFrame(counties_shp, tweet.df, match.ID = "id") 

# library(tmap)
# tm_shape(tweet.polygon) + 
#     tm_polygons(style="quantile", col = "n") +
#     tm_legend(outside = TRUE, text.size = .8) 

# Define neighboring polygons (continguity)
tweet.nb <- poly2nb(tweet.polygon, queen=TRUE)
#tweet.mtx <- nb2mat(tweet.nb, style='B', zero.policy = TRUE)
# assign equal weights to each neighboring polygon
lw <- nb2listw(tweet.nb, style="W", zero.policy=TRUE)
lw$weights[1] # weights of the first polygonâs neighbors

# test for spatial autocorrelation
# Moran's I
moran.test(tweet.polygon$n,lw, zero.policy = TRUE)
# Monte Carlo simulation test for significance
MC <- moran.mc(tweet.polygon$n, lw, nsim=999, zero.policy = TRUE)
MC
plot(MC, main=NULL)

# Geary's C
geary.test(tweet.polygon$n,lw, zero.policy = TRUE)
GC <- geary.mc(tweet.polygon$n, lw, nsim=999, zero.policy = TRUE)
GC
plot(GC, main=NULL)

# apply the same functions to different emotions
moran.test(tweet.polygon$negative, lw, zero.policy = TRUE)
moran.mc(tweet.polygon$negative, lw, nsim=999, zero.policy = TRUE)
geary.test(tweet.polygon$negative, lw, zero.policy = TRUE)
geary.mc(tweet.polygon$negative, lw, nsim=999, zero.policy = TRUE)

moran.test(tweet.polygon$fear, lw, zero.policy = TRUE)
moran.mc(tweet.polygon$fear, lw, nsim=999, zero.policy = TRUE)
geary.test(tweet.polygon$fear, lw, zero.policy = TRUE)
geary.mc(tweet.polygon$fear, lw, nsim=999, zero.policy = TRUE)

moran.test(tweet.polygon$anger, lw, zero.policy = TRUE)
moran.mc(tweet.polygon$anger, lw, nsim=999, zero.policy = TRUE)
geary.test(tweet.polygon$anger, lw, zero.policy = TRUE)
geary.mc(tweet.polygon$anger, lw, nsim=999, zero.policy = TRUE)

moran.test(tweet.polygon$disgust, lw, zero.policy = TRUE)
moran.mc(tweet.polygon$disgust, lw, nsim=999, zero.policy = TRUE)
geary.test(tweet.polygon$disgust, lw, zero.policy = TRUE)
geary.mc(tweet.polygon$disgust, lw, nsim=999, zero.policy = TRUE)

moran.test(tweet.polygon$sadness, lw, zero.policy = TRUE)
moran.mc(tweet.polygon$sadness, lw, nsim=999, zero.policy = TRUE)
geary.test(tweet.polygon$sadness, lw, zero.policy = TRUE)
geary.mc(tweet.polygon$sadness, lw, nsim=999, zero.policy = TRUE)

```
